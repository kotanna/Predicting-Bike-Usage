{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 802: Analytical Tools and Foundations\n",
    "# Homework assignment 1 - Part 2: Predictive Modeling\n",
    "# Last updated: July 30, 2018 by Anna M. Kot\n",
    "\n",
    "# #############################################################################\n",
    "# Import and initialize dependencies.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# #############################################################################\n",
    "# Load and prepare the data for analysis\n",
    "\n",
    "# Set the path to the folder location containing the raw data file 'HW Data.csv', \n",
    "# and load the dataset as 'CBS' for Capital Bikeshare.\n",
    "CBS = pd.read_csv(\"/Users/annakot/Desktop/M.S. Analytics/Data 802/02. HW2/HW Data.csv\")\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame.\n",
    "CBS = pd.DataFrame(CBS)\n",
    "\n",
    "# Drop columns with previously identified NaN >= 1/2 dataset total,\n",
    "# unnecessary 'icon' column,\n",
    "CBS = CBS.drop([\"rain_1h\", \"rain_3h\", \"snow_1h\", \"weather_icon\"], axis=1)\n",
    "\n",
    "CBS.BinCount.replace((\"Low\", \"High\"), (0, 1), inplace=True)\n",
    "\n",
    "# Rename the columns in the CBS DataFrame.\n",
    "CBS = CBS.rename(index=str, columns={\"Date_Key\": \"Date Key\", \n",
    "                                     \"TotalCount\": \"Total Count\", \n",
    "                                     \"BinCount\": \"Bin Count\", \n",
    "                                     \"humidity\": \"Humidity\", \n",
    "                                     \"wind_deg\": \"Wind (Degrees)\",  \n",
    "                                     \"clouds_all\": \"Cloudiness\", \n",
    "                                     \"weather_main\": \"Weather (Main)\",\n",
    "                                     \"Temp_F\": \"Temperature (F)\",\n",
    "                                     \"Temp_Min_F\": \"Minimum Temperature (F)\",\n",
    "                                     \"Temp_Max_F\": \"Maximum Temperature (F)\",\n",
    "                                     \"Wind_MPH\": \"Wind (MPH)\"})\n",
    "\n",
    "# Rearrage the columns in the CBS DataFrame.\n",
    "CBS = CBS[[\"Date Key\", \"Total Count\", \"Bin Count\", \"Humidity\",\n",
    "           \"Cloudiness\", \n",
    "           \"Weather (Main)\", \n",
    "           \"Temperature (F)\", \n",
    "           \"Minimum Temperature (F)\",\n",
    "           \"Maximum Temperature (F)\", \"Wind (Degrees)\",\n",
    "           \"Wind (MPH)\",\"Year\",\"Month\",\"Day\",\"Hour\",\"Holiday\"]]\n",
    "\n",
    "# Create an array, CBSarray, to segment year, month, date, \n",
    "# and hour from the 'Date Key' column in CBS and append to CBSarray.\n",
    "CBSarray = np.array(CBS[\"Date Key\"])\n",
    "year=[]\n",
    "month=[]\n",
    "date=[]\n",
    "hour=[]\n",
    "\n",
    "for i in CBSarray:\n",
    "    j = datetime.datetime.strptime(i, \"%Y-%m-%d-%H\")\n",
    "    year.append(j.year)\n",
    "    month.append(str(j.month).zfill(2))\n",
    "    date.append(str(j.day).zfill(2))\n",
    "    hour.append(str(j.hour).zfill(2))\n",
    "    \n",
    "# Overwrite the existing year, month, day, and hour column, respectively, in \n",
    "# CBS and append segmented year, month, date, and hour from CBSarray to eliminate\n",
    "# null or missing values in the respective columns.    \n",
    "CBS[\"Year\"] = year\n",
    "CBS[\"Month\"] = month\n",
    "CBS[\"Day\"] = date\n",
    "CBS[\"Hour\"] = hour\n",
    "\n",
    "# Convert columns 'Year', 'Month', 'Day', and 'Hour' to int64\n",
    "cols = ['Year','Month','Day','Hour']\n",
    "CBS[cols] = CBS[cols].astype(np.int64)\n",
    "\n",
    "# Print shape of original DataFrame (21715, 18)\n",
    "#print(\"Shape of Original DataFrame: {}\".format(CBS.shape))\n",
    "\n",
    "# Drop missing values (942 missing values)\n",
    "CBS = CBS.dropna()\n",
    "\n",
    "# Print shape of new DataFrame (20773, 18)\n",
    "#print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(CBS.shape))\n",
    "\n",
    "# Generate a heatmap showing the correlation between the different features\n",
    "#import seaborn as sns; sns.set()\n",
    "#sns.heatmap(CBS.corr(), square=True, cmap='RdYlGn')\n",
    "\n",
    "# Drop columns with redundant correlation\n",
    "CBS = CBS.drop([\"Minimum Temperature (F)\", \"Maximum Temperature (F)\"], axis=1)\n",
    "\n",
    "# Generate a heatmap showing the correlation between the different features#\n",
    "#import seaborn as sns; sns.set()\n",
    "#sns.heatmap(CBS.corr(), square=True, cmap='RdYlGn')\n",
    "\n",
    "# Create dummy variables\n",
    "#CBS = pd.get_dummies(CBS, prefix=['Weather (Main)'], columns=['Weather (Main)'])\n",
    "\n",
    "#CBS.corr()\n",
    "\n",
    "# #############################################################################\n",
    "# Create cyclical curves.\n",
    "# This will ensure that the 0 and 23 hour, for example, are close to each other, \n",
    "# thus allowing the cyclical nature of the variable to shine through.\n",
    "\n",
    "# Create and plot cyclical curve for 'Hour': time.\n",
    "CBS = CBS.sort_values('Hour')\n",
    "\n",
    "CBS['sin_time'] = np.sin(2*np.pi*CBS.Hour/24)\n",
    "CBS['cos_time'] = np.cos(2*np.pi*CBS.Hour/24)\n",
    "\n",
    "CBS['time'] = CBS['sin_time'] + CBS['cos_time']\n",
    "\n",
    "#CBS.sin_time.plot();\n",
    "#CBS.cos_time.plot();\n",
    "\n",
    "#CBS.time.plot();\n",
    "#CBS.plot.scatter('sin_time','cos_time').set_aspect('equal');\n",
    "\n",
    "# Create and plot cyclical curve for 'Hour': time.\n",
    "CBS = CBS.sort_values('Month')\n",
    "\n",
    "CBS['sin_month'] = np.sin(2*np.pi*CBS.Month/12)\n",
    "CBS['cos_month'] = np.cos(2*np.pi*CBS.Month/12)\n",
    "\n",
    "CBS['month'] = CBS['sin_month'] + CBS['cos_month']\n",
    "\n",
    "#CBS.sin_month.plot();\n",
    "#CBS.cos_month.plot();\n",
    "\n",
    "#CBS.month.plot();\n",
    "# CBS.plot.scatter('sin_month','cos_month').set_aspect('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Modeling using MULTIPLE LINEAR REGRESSION\n",
    "# Last updated: July 30, 2018 by Anna M. Kot\n",
    "\n",
    "# #############################################################################\n",
    "# Create feature and the response variable arrays.\n",
    "\n",
    "# Define predictor/independent variables.\n",
    "#X = CBS[[\"Humidity\",\"Wind (Degrees)\",\"Year\",\"month\",\"Day\",\"time\",\"Temperature (F)\",\"Wind (MPH)\"]]\n",
    "X = CBS[[\"month\",\"time\",\"Temperature (F)\"]]\n",
    "\n",
    "# Define target/dependent variable.\n",
    "y = CBS['Total Count']\n",
    "\n",
    "# #############################################################################\n",
    "# Determine the most important feature(s) for predictive power.\n",
    "\n",
    "# # Instantiate a lasso regressor.\n",
    "# lasso = Lasso(alpha=0.4, normalize=True)\n",
    "\n",
    "# # Fit the regressor to the data.\n",
    "# lasso.fit(X, y)\n",
    "\n",
    "# # Compute and print the coefficients.\n",
    "# lasso_coef = lasso.coef_\n",
    "# print(lasso_coef)\n",
    "\n",
    "# # Plot the coefficients.\n",
    "# plt.plot(range(len(X.columns)), lasso_coef)\n",
    "# plt.xticks(range(len(X.columns)), X.columns.values, rotation=45)\n",
    "# plt.margins(0.02)\n",
    "# plt.show()\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold, and\n",
    "# prepapre variables for MLR REGRESSION.\n",
    "\n",
    "# Split into a training and testing set using a stratified k fold.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale training and testing predictor/independent variables.\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit_transform(X_test)\n",
    "\n",
    "# #############################################################################\n",
    "# Train a MLR REGRESSION model\n",
    "\n",
    "# Create the regressor and fit the regressor\n",
    "# to the training set.\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = lm.predict(X_test_scaled)\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set.\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(lm.score(X_test_scaled, y_test)))\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Modeling using KNN REGRESSION\n",
    "# Last updated: July 30, 2018 by Anna M. Kot\n",
    "\n",
    "# #############################################################################\n",
    "# Initialize required functions.\n",
    "\n",
    "# class SBS():\n",
    "#     def __init__(self, estimator, k_features,\n",
    "#         scoring=metrics.accuracy_score,\n",
    "#         test_size=0.25, random_state=1):\n",
    "#         self.scoring = scoring\n",
    "#         self.estimator = clone(estimator)\n",
    "#         self.k_features = k_features\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "        \n",
    "#     def fit(self, X, y):\n",
    "#         X_train, X_test, y_train, y_test =\n",
    "#             train_test_split(X, y, test_size=self.test_size,\n",
    "#         random_state=self.random_state)\n",
    "#         dim = X_train.shape[1]\n",
    "#         self.indices_ = tuple(range(dim))\n",
    "#         self.subsets_ = [self.indices_]\n",
    "#         score = self._calc_score(X_train, y_train,\n",
    "#         X_test, y_test, self.indices_)\n",
    "#         self.scores_ = [score]\n",
    "#         while dim > self.k_features:\n",
    "#             scores = []\n",
    "#             subsets = []\n",
    "#             for p in combinations(self.indices_, r=dim-1):\n",
    "#                 score = self._calc_score(X_train, y_train,\n",
    "#                 X_test, y_test, p)\n",
    "#                 scores.append(score)\n",
    "#                 subsets.append(p)\n",
    "#             best = np.argmax(scores)\n",
    "#             self.indices_ = subsets[best]\n",
    "#             self.subsets_.append(self.indices_)\n",
    "#             dim -= 1\n",
    "#             self.scores_.append(scores[best])\n",
    "#         self.k_score_ = self.scores_[-1]\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         return X[:, self.indices_]\n",
    "        \n",
    "#     def _calc_score(self, X_train, y_train,\n",
    "#                         X_test, y_test, indices):\n",
    "#         self.estimator.fit(X_train[:, indices], y_train)\n",
    "#         y_pred = self.estimator.predict(X_test[:, indices])\n",
    "#         score = self.scoring(y_test, y_pred)\n",
    "#         return score\n",
    "\n",
    "# #############################################################################\n",
    "# Create feature and the response variable arrays.\n",
    "\n",
    "# Define predictor/independent variables.\n",
    "X = CBS[[\"Year\",\"month\",\"Day\",\"time\",\"Temperature (F)\",\"Wind (MPH)\"]]\n",
    "\n",
    "# Define target/dependent variable.\n",
    "y = CBS['Total Count']\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set, and\n",
    "# scale variables for KNN REGRESSION.\n",
    "\n",
    "# Split into a training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale training and testing predictor/independent variables.\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit_transform(X_test)\n",
    "\n",
    "# #############################################################################\n",
    "# Determine k using a model complexity curve.\n",
    "\n",
    "# Setup arrays to store, train, and test accuracies.\n",
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "# Loop over different values of k.\n",
    "for i, k in enumerate(neighbors):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn.\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # Fit the classifier to the training set.\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set.\n",
    "    train_accuracy[i] = knn.score(X_train_scaled, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set.\n",
    "    test_accuracy[i] = knn.score(X_test_scaled, y_test)\n",
    "\n",
    "# Generate model complexity curve.\n",
    "plt.title('k-NN: Varying Number of Neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# #############################################################################\n",
    "# Determine k using a GridSearch.\n",
    "# Returns: {'n_neighbors': 1}\n",
    "#          0.27049232941780604\n",
    "\n",
    "# # Specifiy the hyperparameter as a dictionary in which the keys \n",
    "# # are the hyperparameter names.\n",
    "# param_grid = {'n_neighbors': np.arange(1,50)}\n",
    "\n",
    "# # Initiate the KNN classifier.\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# # Using GridSearch, pass in the model, the grid to tune, and the number of folds to use.\n",
    "# knn_cv = GridSearchCV(knn, param_grid, cv=5)\n",
    "\n",
    "# # Perform the GridSearch inplace.\n",
    "# knn_cv.fit(X_train_scaled,y_train)\n",
    "\n",
    "# # Apply the attributes best params and best score, respectively, to retrieve the \n",
    "# # hyperparameters that perform the best along with the mean cross-validation score over that fold.\n",
    "# print(knn_cv.best_params_)\n",
    "# print(knn_cv.best_score_)\n",
    "\n",
    "# #############################################################################\n",
    "# Train a KNN regression model.\n",
    "\n",
    "# Create a KNN classifier with 1 neighbor as determined by the \n",
    "# model complexity curve and the GridSearch.\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Fit (train) the classifier to the training set.\n",
    "KNNmodel = knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set.\n",
    "\n",
    "# Test the response.\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "print(\"Test set predictions: {}\".format(y_pred))\n",
    "\n",
    "# Print the accuracy of the model.\n",
    "print ('Score:', KNNmodel.score(X_test_scaled, y_test))\n",
    "\n",
    "# Print the parameters of the KNN model.\n",
    "print(KNNmodel)\n",
    "\n",
    "# #############################################################################\n",
    "# Sequential backward selection (SBS) to select the top features \n",
    "# with most predictive power.\n",
    "\n",
    "# from sklearn.base import clone\n",
    "# from itertools import combinations\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# sbs = SBS(knn, k_features=1)\n",
    "# sbs.fit(X_train_scaled, y_train)\n",
    "\n",
    "# k_feat = [len(k) for k in sbs.subsets_]\n",
    "# plt.plot(k_feat, sbs.scores_, marker='o')\n",
    "# #plt.ylim([0.7, 1.1])\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Number of features')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# print(sbs.subsets_)\n",
    "# k5 = list(sbs.subsets_[3])\n",
    "# print(X.columns[1:][k5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8921832884097035\n",
      "[[0.021 0.979]\n",
      " [0.098 0.902]\n",
      " [0.121 0.879]\n",
      " ...\n",
      " [0.139 0.861]\n",
      " [0.015 0.985]\n",
      " [0.14  0.86 ]]\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          1303   246  1549\n",
      "1           314  3331  3645\n",
      "All        1617  3577  5194\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.84      0.82      1549\n",
      "          1       0.93      0.91      0.92      3645\n",
      "\n",
      "avg / total       0.89      0.89      0.89      5194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictive Modeling using LOGISTIC REGRESSION\n",
    "# Last updated: July 30, 2018 by Anna M. Kot\n",
    "\n",
    "# #############################################################################\n",
    "# Create feature and the response variable arrays.\n",
    "\n",
    "# Define predictor/independent variables.\n",
    "#X = CBS[[\"Humidity\",\"Wind (Degrees)\",\"Year\",\"month\",\"Day\",\"time\",\"Temperature (F)\",\"Wind (MPH)\"]]\n",
    "X = CBS[[\"Wind (Degrees)\",\"month\",\"time\",\"Temperature (F)\",\"Wind (MPH)\"]]\n",
    "\n",
    "# Define target/dependent variable.\n",
    "y = CBS[\"Bin Count\"]\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set, and\n",
    "# scale variables for LOGISTIC REGRESSION.\n",
    "\n",
    "# Split into a training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Scale training and testing predictor/independent variables.\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit_transform(X_test)\n",
    "\n",
    "# #############################################################################\n",
    "# Determine regression parameters using a GridSearch.\n",
    "# C results in 0.05179474679231213 for my model.\n",
    "\n",
    "# # Setup the hyperparameter grid\n",
    "# c_space = np.logspace(-5, 8, 15)\n",
    "# param_grid = {'C': c_space}\n",
    "\n",
    "# # Instantiate a logistic regression classifier: logreg\n",
    "# logreg = LogisticRegression()\n",
    "\n",
    "# # Instantiate the GridSearchCV object: logreg_cv\n",
    "# logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# # Fit it to the data\n",
    "# logreg_cv.fit(X_train_scaled,y_train)\n",
    "\n",
    "# # Print the tuned parameter and score\n",
    "# print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "# print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# #############################################################################\n",
    "# Recursive feature elimination to select the top three features.\n",
    "\n",
    "# from sklearn.feature_selection import RFE\n",
    "# rfe = RFE(model, 3)\n",
    "# fit = rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# #print(fit.n_features_)\n",
    "# print(fit.support_)\n",
    "# print(fit.ranking_)\n",
    "\n",
    "# #############################################################################\n",
    "# Train a LOGISTIC REGRESSION model\n",
    "\n",
    "# Create a logistic classifier and fit the classifier to the training set.\n",
    "model = LogisticRegression(C=0.052)\n",
    "model = model.fit (X_train_scaled, y_train)\n",
    "\n",
    "# Predict the class labels for the test set.\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set.\n",
    "\n",
    "# Print the accuracy of the model.\n",
    "print ('Score:', model.score(X_test_scaled, y_test))\n",
    "\n",
    "# Calculate the probabilities of the class for the test set.\n",
    "probability = model.predict_proba(X_test_scaled)\n",
    "print (probability)\n",
    "\n",
    "# Calculate the confusion matrix to describe the performance of\n",
    "# the classification model on a set of test data for which the\n",
    "# true values are known.\n",
    "print (pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "# Classification report is another method to examine the \n",
    "# performance of the classification model.\n",
    "print (metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# #############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib.\n",
    "\n",
    "# Compute predicted probabilities.\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds.\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.show()\n",
    "\n",
    "# Compute and print AUC score.\n",
    "#print(\"AUC: {}\".format(metrics.roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Compute cross-validated AUC scores:.\n",
    "#cv_auc = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "#print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Modeling using NAIVE BAYES CLASSIFICATION\n",
    "# Last updated: July 30, 2018 by Anna M. Kot\n",
    "\n",
    "# #############################################################################\n",
    "# Create feature and the response variable arrays.\n",
    "\n",
    "# Define predictor/independent variables.\n",
    "X = CBS[[\"Wind (Degrees)\",\"month\",\"time\",\"Temperature (F)\",\"Wind (MPH)\"]]\n",
    "\n",
    "# Define target/dependent variable.\n",
    "y = CBS[\"Bin Count\"]\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set, and\n",
    "# scale variables for LOGISTIC REGRESSION.\n",
    "\n",
    "# Split into a training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Scale training and testing predictor/independent variables.\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit_transform(X_test)\n",
    "\n",
    "# #############################################################################\n",
    "# Train a NAIVE BAYES CLASSIFICATION model\n",
    "\n",
    "# Create a naive bayes classifier and fit the \n",
    "# classifier to the training set.\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test_scaled)\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the predictions using matplotlib.\n",
    "\n",
    "# FUNCTIONS\n",
    "def success_ratio(cm):\n",
    "    total = cm[0][0] + cm[1][0] + cm[0][1] + cm[1][1]\n",
    "    return 100*(cm[0][0] + cm[1][1]) / total\n",
    "\n",
    "# Print the accuracy of the model.\n",
    "print ('Score:', gnb.score(X_test_scaled, y_test))\n",
    "\n",
    "# Confusion matrix.\n",
    "cm_test = pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "# Classification report is another method to examine the \n",
    "# performance of the classification model.\n",
    "print (metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Test set confusion matrix : \\n\"+str(cm_test))\n",
    "print(\"Success ratio on test set : \"+str(success_ratio(cm=cm_test))+\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # empty list that will hold cv scores\n",
    "# cv_scores = []\n",
    "\n",
    "# for k in neighbors:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     scores = cross_val_score(knn, X_train_scaled, y_train, cv=10, scoring='r2')\n",
    "#     cv_scores.append(scores.mean())\n",
    "    \n",
    "# # changing to misclassification error\n",
    "# MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# # determining best k\n",
    "# optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "# print (optimal_k)\n",
    "\n",
    "# # plot misclassification error vs k\n",
    "# plt.plot(neighbors, MSE)\n",
    "# plt.xlabel('Number of Neighbors K')\n",
    "# plt.ylabel('Misclassification Error')\n",
    "# plt.show()\n",
    "\n",
    "# print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.DataFrame(X\n",
    "Y = pd.DataFrame(y)\n",
    "                 \n",
    "                 \n",
    "#P = pd.DataFrame(y_pred)\n",
    "\n",
    "#df = (X, Y, P)\n",
    "#print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4  Wind (Degrees)  month  time  Temperature (F)  \\\n",
      "0 NaN NaN NaN NaN NaN             NaN    NaN   NaN              NaN   \n",
      "1 NaN NaN NaN NaN NaN             NaN    NaN   NaN              NaN   \n",
      "2 NaN NaN NaN NaN NaN             NaN    NaN   NaN              NaN   \n",
      "3 NaN NaN NaN NaN NaN             NaN    NaN   NaN              NaN   \n",
      "\n",
      "   Wind (MPH)  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "mean_of_array = X.mean(axis=0)\n",
    "std_of_array = X.std(axis=0)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled)\n",
    "\n",
    "X_original = (X_test_scaled * std_of_array) + mean_of_array\n",
    "\n",
    "#print (X[:4])\n",
    "print (X_original[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "\n",
    "X_test_scaled.to_csv('/Users/annakot/Desktop/1.csv', index=False)\n",
    "y_test.to_csv('/Users/annakot/Desktop/2.csv', index=False)\n",
    "y_pred.to_csv('/Users/annakot/Desktop/3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wind (Degrees)     105.258490\n",
       "month                0.980286\n",
       "time                 0.996787\n",
       "Temperature (F)     16.723982\n",
       "Wind (MPH)           4.406127\n",
       "dtype: float64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_of_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
